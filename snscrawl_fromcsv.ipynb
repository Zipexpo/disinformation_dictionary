{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas\n",
    "from getTopic import getTopic,getTopicv2\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def getUrl(source,_from,_to):\n",
    "    tweets_list2 = []\n",
    "    query = f'from:{source} since:{_from} until:{_to}'\n",
    "    for i,tweet in enumerate(sntwitter.TwitterSearchScraper(query).get_items()):\n",
    "        tweets_list2.append([tweet.id,tweet.user.id,tweet.user.username,tweet.user.displayname,tweet.user.location,tweet.user.description,tweet.user.descriptionUrls,tweet.content,tweet.date.strftime(\"%Y-%m-%d %H:%M:%S\"),tweet.hashtags,tweet.inReplyToTweetId,tweet.inReplyToUser,tweet.user.followersCount,tweet.user.followersCount,tweet.user.created,tweet.lang,tweet.retweetCount,\n",
    "                             tweet.likeCount,tweet.lang])\n",
    "    return pd.DataFrame(tweets_list2,columns=['tweetid','userid','user_display_name','user_screen_name','user_reported_location','user_profile_description','user_profile_url','tweet_text','tweet_time','hashtags','in_reply_to_tweetid','in_reply_to_userid','follower_count','following_count','account_creation_date','account_language','retweet_count',\n",
    "                                              'like_count','tweet_language'])\n",
    "    # return sntwitter.TwitterSearchScraper(f'from:{source} since:{_from} until:{_to}').get_items()\n",
    "\n",
    "def getUrl2(source,_from,_to):\n",
    "    tweets_list2 = []\n",
    "    query = f'from:{source} since:{_from} until:{_to}'\n",
    "    for i,tweet in enumerate(sntwitter.TwitterSearchScraper(query).get_items()):\n",
    "        tweets_list2.append(tweet)\n",
    "    return tweets_list2\n",
    "    # return sntwitter.TwitterSearchScraper(f'from:{source} since:{_from} until:{_to}').get_items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "fileName = 'FNsample4'\n",
    "df = pd.read_csv(\"../../../data/raw/excel_files/\"+fileName+\".csv\")\n",
    "df['tweet_time'] = pd.to_datetime(df['tweet_time'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "2016-09-26 2016-10-02\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "timeRange = list(df.groupby([df['tweet_time'].dt.date]).groups.keys())\n",
    "_from = timeRange[0]\n",
    "_to = timeRange[-1]\n",
    "print(_from,_to)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "from getTopic import handleData\n",
    "from getTopic import df2tweet\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime,timedelta\n",
    "import json\n",
    "\n",
    "listNews = pd.read_excel(\"data/News List and dictionary.xlsx\",sheet_name='News list')\n",
    "listNews = listNews[pd.isna(listNews[\"Type\"])]\n",
    "\n",
    "def getFromsource(_from,_to):\n",
    "    for i in tqdm(range(0,df_raw.shape[0])):\n",
    "        try:\n",
    "            [top_n_topics_df,keywords_appearing_df] = getTopic(None,df2tweet(df_raw.iloc[i]))\n",
    "            result.append([top_n_topics_df,keywords_appearing_df])\n",
    "            # if(top_n_topics_df['score1'].values[0] != 0):\n",
    "            #     print(i, top_n_topics_df['top1'].values, top_n_topics_df['score1'].values[0])\n",
    "        except:\n",
    "            result.append([None])\n",
    "            print('fail')\n",
    "        # clear_output(wait=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "# _df = getUrl(listNews.iloc[2]['Twitter Link'].split('/')[-1],list(df.groupby([df['tweet_time'].dt.date]).groups.keys())[0],list(df.groupby([df['tweet_time'].dt.date]).groups.keys())[0]+ timedelta(days=1))\n",
    "# [top_n_topics_df,keywords_appearing_df] = getTopic(None,df2tweet(_df.iloc[0]))\n",
    "# _df.iloc[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=61880.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "267d7e5e9d8f4606a284b3d611a99649"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": [
      "fail\n",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# 492,509, 520, 539, 560,576\n",
    "result = []\n",
    "df_raw = pd.read_csv('data/'+fileName+'_real.csv')\n",
    "getFromsource(_from,_to)\n",
    "df2 = df_raw.loc[:,['tweetid','userid','user_display_name','tweet_time']]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "def addValue(df2,i,keyIndex):\n",
    "    if((result[i] != None)):\n",
    "        if(len(result[i])==2):\n",
    "            c = result[i][0]['top'+str(keyIndex)].values[0]\n",
    "            df2.at[i,c] = result[i][0]['score'+str(keyIndex)].values[0]\n",
    "            df2.at[i,c+\"_keywords\"] = str(result[i][1][c+'_keywords'].values[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=61880.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ab1b2a631f29492cb7fa38a58b3ff4a6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": [
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "topics_cols=[]\n",
    "with open(\"data/keywords.json\", \"r\") as read_file:\n",
    "    topics_cols =  list(json.load(read_file))\n",
    "for c in topics_cols:\n",
    "    df2[c]=0\n",
    "    df2[c+\"_keywords\"]='()'\n",
    "\n",
    "for i in tqdm(range(0,df_raw.shape[0])):\n",
    "    addValue(df2,i,1)\n",
    "    addValue(df2,i,2)\n",
    "    addValue(df2,i,3)\n",
    "    addValue(df2,i,4)\n",
    "    addValue(df2,i,5)\n",
    "df2.to_csv('data/'+fileName+'_real_topic.csv',index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "data": {
      "text/plain": "[      top1               top2               top3   top4    top5  score1  \\\n 0  Traffic  Weather (Regular)  Weather (Extreme)  Crime  Events     0.0   \n \n    score2  score3  score4  score5  \n 0     0.0     0.0     0.0     0.0  ,\n   Traffic_keywords Weather (Regular)_keywords Weather (Extreme)_keywords  \\\n 0               ()                         ()                         ()   \n \n   Crime_keywords Events_keywords Lifestyle_keywords Entertainment_keywords  \\\n 0             ()              ()                 ()                     ()   \n \n   Sports_keywords Education_keywords Politics_keywords  ... Leisure_keywords  \\\n 0              ()                 ()                ()  ...               ()   \n \n   Obituaries_keywords Abortion_keywords 2nd Amendment_keywords  \\\n 0                  ()                ()                     ()   \n \n   Immigration_keywords Police Brutality_keywords Terrorism_keywords  \\\n 0                   ()                        ()                 ()   \n \n   Race_keywords Religion_keywords Russian Sphere_keywords  \n 0            ()                ()                      ()  \n \n [1 rows x 30 columns]]"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 55
    }
   ],
   "source": [
    "result[346]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}